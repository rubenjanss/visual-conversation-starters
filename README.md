# Visual Conversation Starters

This repository contains the dataset and code associated with the paper *"Cool glasses, where did you get them?": Generating Visually Grounded conversation starters for Human-Robot Dialogue*, by Ruben Janssens, Pieter Wolfert, Thomas Demeester, and Tony Belpaeme, published at HRI'22. **Coming soon: LINK**

In this work, we built a system that generates questions that robots can use to start an open-domain conversation with a user, based on visual information.

We collected a data set of 4000 images that are appropriate for Human-Robot Interaction (HRI), meaning they are similar to what a robot camera would see in an interaction with a user. Each image is accompanied by three conversation-starting questions, that refer to something in the image.

Then, we compared two systems to generate these questions: a retrieval-based model as baseline, and the Transformer-based encoder-decoder model BART, fine-tuned on our data set. Both models were trained and evaluated using ParlAI. As input for the question-generating models, we used a dense captioning model to generate a description of the image.

Finally, we deployed the system on a Furhat social robot.

This README contains technical instructions to replicate our results. Read our **blog post** (**LINK coming soon**) for more information, or the **paper** itself if you want details about the methodology! For any further questions, do not hesitate to contact `rmajanss[dot]janssens[at]ugent[dot]be` or [@rubenjanss](https://www.twitter.com/rubenjanss)  on Twitter.

If you use our work, please cite our paper using the following citation:
```
Coming soon!
```

## Data set

`data/` contains the full data set. The images are selected from the [YFCC100M data set](http://www.multimediacommons.org/) and are not hosted in this repository, rather, we refer to the images using their Flickr URL.

The data set is split into a training set (3k images), validation set (500 images), and test set (500 images). Each subset is represented as a JSON file with the following columns, each with one entry per image:

- **Answer.question1**: first question associated with the image
- **Answer.question2**: second question associated with the image
- **Answer.question3**: third question associated with the image
- **url**: Flickr URL of the image
- **name**: filename of the image (substring of the URL)
- **dot_string**: description of the image, generated by the dense captioning model.

We have also included a modified version of the training set, called `
train_dataset_lowfreq.json `. This data set contains all images and questions of the original training set, *except* for the six questions that were most common in the training set, each occurring more than 50 times. We found that the system generated more diverse questions after being trained on this reduced data set.

## Captioning

The image descriptions were generated by the *Dense Captioning with Joint Inference and Visual Context* model, from [https://github.com/linjieyangsc/densecap](https://github.com/linjieyangsc/densecap). We used their official sample model and did not fine-tune it further. The model ran in a machine with an NVIDIA GeForce GTX 1080 Ti GPU with 11GB VRAM.

`captioning/` contains code that we used for running the captioning model.

- `test.py` is a modified version of `lib/fast_rcnn/test.py` in the densecap repo: replace this file with our file.
- `batch.py` is a modified version of `lib/tools/demo.py` for generating captions for a batch of images.

- `caption_server.py` is a modified version of `lib/tools/demo.py` for live caption generation, e.g. when running the system with Furhat.

Run `batch.py` with the following arguments: `python batch.py --image_folder <IMAGE_FOLDER> --output <OUTPUT_FILE> --image_file <DATASET_FILE>`. `DATASET_FILE` is a JSON file with a data set containing the `url` and `name` fields as in our data set. The script will download all images into `IMAGE_FOLDER`.

## Training and Evaluating using ParlAI

`parlai_internal/` 

based on MNIST teacher (link)

## Running on Furhat

<script> window.scroll(0,200000) </script> 